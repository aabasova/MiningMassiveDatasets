{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72051a3",
   "metadata": {},
   "source": [
    "# Problem Set 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4424f",
   "metadata": {},
   "source": [
    "Team: \n",
    "    Lea Schmierer 3546563;\n",
    "    Angelina Basova 3704658;\n",
    "    Daniel Knorr 3727033"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85055ee5",
   "metadata": {},
   "source": [
    "## Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b48ab",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606fb41",
   "metadata": {},
   "source": [
    "#### a) Explain briefly why all nodes in the clique have the same PageRank value.\n",
    "\n",
    "All nodes in the clique have the same value, because they all have the same number of in-links and out-links, therefore none of these nodes are more important than the others. Every note is weighted the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea59239",
   "metadata": {},
   "source": [
    "#### b) \n",
    "You can find the solution of this exercise in the zip file as a pdf. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f246a6",
   "metadata": {},
   "source": [
    "#### c)\n",
    "You can find the solution of this exercise in the zip file as a pdf. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275817d",
   "metadata": {},
   "source": [
    "#### d) Explain briefly why random teleports not only solve the spider trap problem but also the dead ends problem. \n",
    "\n",
    "There are two approaches to dealing with dead ends: \n",
    "- We can drop the dead ends from the graph and their incoming arcs.\n",
    "- We can modify the process by which random surfers are assumed to move about the web. This approach also resolves the problem of spider traps. It is also called taxation\n",
    "\n",
    "\n",
    "Solution: \n",
    "The previous procedure describes how the columns of a matrix M were made stochastic so that teleportation can always be done when there is nowhere else to go.\n",
    "\n",
    "vom BUCH:\n",
    "\n",
    "\"Note that if the graph has no dead ends, then the probability of introducing a\n",
    "new random surfer is exactly equal to the probability that the random surfer will\n",
    "decide not to follow a link from their current page. In this case, it is reasonable\n",
    "to visualize the surfer as deciding either to follow a link or teleport to a random\n",
    "page. However, if there are dead ends, then there is a third possibility, which\n",
    "is that the surfer goes nowhere. Since the term (1 − )e/n does not depend on\n",
    "the sum of the components of the vector v, there will always be some fraction\n",
    "of a surfer operating on the Web. That is, when there are dead ends, the sum\n",
    "of the components of v may be less than 1, but it will never reach 0.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172ffa9",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2699d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3a)\n",
      "Matrix M:\n",
      "[[0.5 0.5 0. ]\n",
      " [0.5 0.  0. ]\n",
      " [0.  0.5 1. ]]\n",
      "\n",
      "Number of iterations: 12\n",
      "\n",
      "Pagerank results:\n",
      "[[0.2130343 ]\n",
      " [0.15207948]\n",
      " [0.63488622]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "######################\n",
    "# 3a)\n",
    "print(\"Exercise 3a)\")\n",
    "######################\n",
    "\n",
    "\n",
    "# return matrix A\n",
    "def get_google_matrix(M, beta):\n",
    "    scalar = np.ones(shape=M.shape) * 1 / M.shape[0]\n",
    "    A = beta * M + (1-beta) * scalar \n",
    "    return A\n",
    "\n",
    "def can_terminate(difference, epsilon):\n",
    "    return np.all(difference < epsilon)\n",
    "\n",
    "\n",
    "def calculatePageRank(M, beta, epsilon):\n",
    "    pagerank = []\n",
    "\n",
    "    A = get_google_matrix(M, beta)\n",
    "\n",
    "    # init rank r\n",
    "    num_nodes = M.shape[0]\n",
    "    # r_0\n",
    "    old_r = np.full((num_nodes, 1), 1/num_nodes)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        new_r = np.dot(A, old_r)\n",
    "        difference = np.abs(new_r - old_r)\n",
    "\n",
    "        old_r = new_r\n",
    "\n",
    "        count +=1\n",
    "\n",
    "        # if termination criterion fulfilled stop power iteration\n",
    "        if can_terminate(difference, epsilon):\n",
    "            pagerank = new_r\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    print(f\"Number of iterations: {count}\")\n",
    "    return pagerank\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    matrix = np.matrix([[1/2, 1/2,0],[1/2, 0,0], [0, 1/2, 1]])\n",
    "    print(\"Matrix M:\")\n",
    "    print(matrix)\n",
    "    \n",
    "    pagerank = calculatePageRank(matrix, 0.8, 0.001 )\n",
    "    print()\n",
    "    print(\"Pagerank results:\")\n",
    "    print(pagerank)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea6919b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3b)\n",
      "\n",
      "Clique with 4 nodes\n",
      "[[0.         0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.         0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.         0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333 0.        ]]\n",
      "\n",
      "Number of iterations: 1\n",
      "Pagerank\n",
      "[[0.25]\n",
      " [0.25]\n",
      " [0.25]\n",
      " [0.25]]\n",
      "-------------------------------------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "Clique with 6 nodes\n",
      "[[0.  0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.  0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.  0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.  0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.  0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2 0. ]]\n",
      "\n",
      "Number of iterations: 1\n",
      "Pagerank\n",
      "[[0.25]\n",
      " [0.25]\n",
      " [0.25]\n",
      " [0.25]]\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# 3b)\n",
    "print(\"Exercise 3b)\")\n",
    "######################\n",
    "\n",
    "def generate_matrix(n:int):\n",
    "    M = np.full((n, n), 1/(n-1))\n",
    "    np.fill_diagonal(M, 0)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print()\n",
    "    M_4 = generate_matrix(4)\n",
    "    print(\"Clique with 4 nodes\")\n",
    "    print(M_4)\n",
    "    pagerank_4 = calculatePageRank(M_4, 0.8, 1/12)\n",
    "    print(\"Pagerank\")\n",
    "    print(pagerank_4)\n",
    "\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(\"Clique with 6 nodes\")\n",
    "    M_6 = generate_matrix(6)\n",
    "    print(M_6)\n",
    "    pagerank_6 = calculatePageRank(M_6, 0.8, 1/12)\n",
    "    print(\"Pagerank\")\n",
    "    print(pagerank_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8746ce",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b48b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType\n",
    "\n",
    "\n",
    "######################\n",
    "# 4a)\n",
    "print(\"Exercise 4a)\")\n",
    "######################\n",
    "\n",
    "sparkSession = SparkSession.builder.appName('A4E4').getOrCreate()\n",
    "\n",
    "print(\"# --------------------------------\")\n",
    "print(\"# Create DataFrame DF1 webStanfordDF\")\n",
    "print(\"# --------------------------------\")\n",
    "\n",
    "rdd_datafile = sparkSession.sparkContext.textFile('./web-Stanford_small.txt')\n",
    "\n",
    "rdd_web_standford = rdd_datafile.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "webStanfordRDD = rdd_web_standford.map(lambda p: (\n",
    "    int(p[0]), int(p[1].strip())))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FromNodeId\", IntegerType(), True),\n",
    "    StructField(\"ToNodeId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "webStanfordDF = sparkSession.createDataFrame(webStanfordRDD, schema)\n",
    "\n",
    "# Get all Nodes as a Set\n",
    "listFromNodeId = webStanfordDF.select(webStanfordDF[\"FromNodeId\"]).toPandas()['FromNodeId']\n",
    "listeToNodeId = webStanfordDF.select(webStanfordDF[\"ToNodeId\"]).toPandas()['ToNodeId']\n",
    "\n",
    "listFromNodeId = list(listFromNodeId)\n",
    "listeToNodeId = list(listeToNodeId)\n",
    "\n",
    "allNodesList = listFromNodeId + listeToNodeId\n",
    "allNodesSet = set(allNodesList)\n",
    "\n",
    "# Define dictionary\n",
    "dic = {}\n",
    "\n",
    "def generateDictionary(allNodesSet, webStanfordDF, dic):\n",
    "    for node in allNodesSet:\n",
    "        liste = []\n",
    "        filteredByFromNodeId = webStanfordDF.filter(webStanfordDF[\"FromNodeId\"] == node)\n",
    "        listOutgoingLinksTo = filteredByFromNodeId.select(filteredByFromNodeId[\"ToNodeId\"]).toPandas()['ToNodeId']\n",
    "        filteredByToNodeId = webStanfordDF.filter(webStanfordDF[\"ToNodeId\"] == node)\n",
    "        listIngoingLinksFrom = filteredByToNodeId.select(filteredByToNodeId[\"FromNodeId\"]).toPandas()[\n",
    "            'FromNodeId']\n",
    "        listOutgoingLinksTo = list(listOutgoingLinksTo)\n",
    "        listIngoingLinksFrom = list(listIngoingLinksFrom)\n",
    "        liste.append(listOutgoingLinksTo)\n",
    "        liste.append(listIngoingLinksFrom)\n",
    "        dic[node] = liste\n",
    "\n",
    "\n",
    "generateDictionary(allNodesSet, webStanfordDF, dic)\n",
    "\n",
    "print(\"Dictionary Composition: Node i: <[Out-neighbors of i]>,<[In-neighbors of i]>\")\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "sparkSession = SparkSession.builder.appName('A4E4').getOrCreate()\n",
    "\n",
    "######################\n",
    "# 4b)\n",
    "print(\"Exercise 4b)\")\n",
    "######################\n",
    "\n",
    "rdd_datafile = sparkSession.sparkContext.textFile('./web-Stanford_small.txt')\n",
    "\n",
    "rdd_web_standford = rdd_datafile.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FromNodeId\", StringType(), False),\n",
    "    StructField(\"ToNodeId\", StringType(), False)\n",
    "])\n",
    "\n",
    "webStanfordDF = sparkSession.createDataFrame(rdd_web_standford, schema)\n",
    "\n",
    "#create FromNode_list\n",
    "FromNode_list = webStanfordDF.rdd.map(lambda x: x.FromNodeId).collect()\n",
    "\n",
    "# create ToNode_set\n",
    "ToNode_list = webStanfordDF.rdd.map(lambda x: x.ToNodeId).collect()\n",
    "ToNode_set = set(ToNode_list)\n",
    "\n",
    "# die ganz Toten finden (die, die nicht woanders hinzeigen)\n",
    "dead_ends = set()\n",
    "\n",
    "for ToNode in ToNode_set:\n",
    "    if ToNode not in FromNode_list:\n",
    "        dead_ends.add(ToNode)\n",
    "\n",
    "# die Toten finden, die nur auf die Toten zeigen\n",
    "# python gesamtliste mit Pandas erzeugen\n",
    "pdframe = webStanfordDF.toPandas()\n",
    "gesamtliste = list(pdframe.values.tolist())\n",
    "gesamtliste.sort()\n",
    "\n",
    "neuToteSuchen = True\n",
    "while neuToteSuchen:\n",
    "    neue_dead_ends = set()\n",
    "    FromNodeOld = \"\"\n",
    "    countNotDead = 0\n",
    "    for x in gesamtliste:\n",
    "        FromNode = x[0]\n",
    "        ToNode = x[1]\n",
    "        if FromNode != FromNodeOld:  # gruppenwechsel\n",
    "            if FromNodeOld != \"\":    # beim erstem mal nicht\n",
    "                if countNotDead == 0:\n",
    "                    dead_ends.add(FromNodeOld)\n",
    "                    neue_dead_ends.add(FromNodeOld)\n",
    "            FromNodeOld = FromNode\n",
    "            countNotDead = 0\n",
    "        if ToNode not in dead_ends:\n",
    "            countNotDead = countNotDead + 1\n",
    "\n",
    "    if len(neue_dead_ends) == 0:\n",
    "        neuToteSuchen = False   # keine mehr gefunden - das wars\n",
    "\n",
    "    workliste = [x for x in gesamtliste if x[0] not in neue_dead_ends]\n",
    "    gesamtliste = workliste\n",
    "\n",
    "print(\"Anzahl DEAD-ENDS: \", len(dead_ends))\n",
    "print(\"Dead End Liste: \", dead_ends)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
