{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72051a3",
   "metadata": {},
   "source": [
    "# Problem Set 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4424f",
   "metadata": {},
   "source": [
    "Team: \n",
    "    Lea Schmierer 3546563;\n",
    "    Angelina Basova 3704658;\n",
    "    Daniel Knorr 3727033"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85055ee5",
   "metadata": {},
   "source": [
    "## Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b48ab",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606fb41",
   "metadata": {},
   "source": [
    "#### a) Explain briefly why all nodes in the clique have the same PageRank value.\n",
    "\n",
    "All nodes in the clique have the same value, because they all have the same number of in-links and out-links, therefore none of these nodes are more important than the others. Every note is weighted the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea59239",
   "metadata": {},
   "source": [
    "#### b) \n",
    "The Google Matrix A: A= beta*M+(1-beta)[1/N] with beta = probability to follow a link at random\n",
    "\n",
    "Now: beta= 1 ??????? -> ODER???? oder ist 1-beta= 1????????\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f246a6",
   "metadata": {},
   "source": [
    "#### c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275817d",
   "metadata": {},
   "source": [
    "#### d) Explain briefly why random teleports not only solve the spider trap problem but also the dead ends problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172ffa9",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8746ce",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b48b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType\n",
    "\n",
    "\n",
    "######################\n",
    "# 4a)\n",
    "print(\"Exercise 4a)\")\n",
    "######################\n",
    "\n",
    "sparkSession = SparkSession.builder.appName('A4E4').getOrCreate()\n",
    "\n",
    "print(\"# --------------------------------\")\n",
    "print(\"# Create DataFrame DF1 webStanfordDF\")\n",
    "print(\"# --------------------------------\")\n",
    "\n",
    "rdd_datafile = sparkSession.sparkContext.textFile('./web-Stanford_small.txt')\n",
    "\n",
    "rdd_web_standford = rdd_datafile.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "webStanfordRDD = rdd_web_standford.map(lambda p: (\n",
    "    int(p[0]), int(p[1].strip())))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FromNodeId\", IntegerType(), True),\n",
    "    StructField(\"ToNodeId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "webStanfordDF = sparkSession.createDataFrame(webStanfordRDD, schema)\n",
    "\n",
    "# Get all Nodes as a Set\n",
    "listFromNodeId = webStanfordDF.select(webStanfordDF[\"FromNodeId\"]).toPandas()['FromNodeId']\n",
    "listeToNodeId = webStanfordDF.select(webStanfordDF[\"ToNodeId\"]).toPandas()['ToNodeId']\n",
    "\n",
    "listFromNodeId = list(listFromNodeId)\n",
    "listeToNodeId = list(listeToNodeId)\n",
    "\n",
    "allNodesList = listFromNodeId + listeToNodeId\n",
    "allNodesSet = set(allNodesList)\n",
    "\n",
    "# Define dictionary\n",
    "dic = {}\n",
    "\n",
    "def generateDictionary(allNodesSet, webStanfordDF, dic):\n",
    "    for node in allNodesSet:\n",
    "        liste = []\n",
    "        filteredByFromNodeId = webStanfordDF.filter(webStanfordDF[\"FromNodeId\"] == node)\n",
    "        listOutgoingLinksTo = filteredByFromNodeId.select(filteredByFromNodeId[\"ToNodeId\"]).toPandas()['ToNodeId']\n",
    "        filteredByToNodeId = webStanfordDF.filter(webStanfordDF[\"ToNodeId\"] == node)\n",
    "        listIngoingLinksFrom = filteredByToNodeId.select(filteredByToNodeId[\"FromNodeId\"]).toPandas()[\n",
    "            'FromNodeId']\n",
    "        listOutgoingLinksTo = list(listOutgoingLinksTo)\n",
    "        listIngoingLinksFrom = list(listIngoingLinksFrom)\n",
    "        liste.append(listOutgoingLinksTo)\n",
    "        liste.append(listIngoingLinksFrom)\n",
    "        dic[node] = liste\n",
    "\n",
    "\n",
    "generateDictionary(allNodesSet, webStanfordDF, dic)\n",
    "\n",
    "print(\"Dictionary Composition: Node i: <[Out-neighbors of i]>,<[In-neighbors of i]>\")\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "sparkSession = SparkSession.builder.appName('A4E4').getOrCreate()\n",
    "\n",
    "######################\n",
    "# 4b)\n",
    "print(\"Exercise 4b)\")\n",
    "######################\n",
    "\n",
    "rdd_datafile = sparkSession.sparkContext.textFile('./web-Stanford_small.txt')\n",
    "\n",
    "rdd_web_standford = rdd_datafile.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FromNodeId\", StringType(), False),\n",
    "    StructField(\"ToNodeId\", StringType(), False)\n",
    "])\n",
    "\n",
    "webStanfordDF = sparkSession.createDataFrame(rdd_web_standford, schema)\n",
    "\n",
    "#create FromNode_list\n",
    "FromNode_list = webStanfordDF.rdd.map(lambda x: x.FromNodeId).collect()\n",
    "\n",
    "# create ToNode_set\n",
    "ToNode_list = webStanfordDF.rdd.map(lambda x: x.ToNodeId).collect()\n",
    "ToNode_set = set(ToNode_list)\n",
    "\n",
    "# die ganz Toten finden (die, die nicht woanders hinzeigen)\n",
    "dead_ends = set()\n",
    "\n",
    "for ToNode in ToNode_set:\n",
    "    if ToNode not in FromNode_list:\n",
    "        dead_ends.add(ToNode)\n",
    "\n",
    "# die Toten finden, die nur auf die Toten zeigen\n",
    "# python gesamtliste mit Pandas erzeugen\n",
    "pdframe = webStanfordDF.toPandas()\n",
    "gesamtliste = list(pdframe.values.tolist())\n",
    "gesamtliste.sort()\n",
    "\n",
    "neuToteSuchen = True\n",
    "while neuToteSuchen:\n",
    "    neue_dead_ends = set()\n",
    "    FromNodeOld = \"\"\n",
    "    countNotDead = 0\n",
    "    for x in gesamtliste:\n",
    "        FromNode = x[0]\n",
    "        ToNode = x[1]\n",
    "        if FromNode != FromNodeOld:  # gruppenwechsel\n",
    "            if FromNodeOld != \"\":    # beim erstem mal nicht\n",
    "                if countNotDead == 0:\n",
    "                    dead_ends.add(FromNodeOld)\n",
    "                    neue_dead_ends.add(FromNodeOld)\n",
    "            FromNodeOld = FromNode\n",
    "            countNotDead = 0\n",
    "        if ToNode not in dead_ends:\n",
    "            countNotDead = countNotDead + 1\n",
    "\n",
    "    if len(neue_dead_ends) == 0:\n",
    "        neuToteSuchen = False   # keine mehr gefunden - das wars\n",
    "\n",
    "    workliste = [x for x in gesamtliste if x[0] not in neue_dead_ends]\n",
    "    gesamtliste = workliste\n",
    "\n",
    "print(\"Anzahl DEAD-ENDS: \", len(dead_ends))\n",
    "print(\"Dead End Liste: \", dead_ends)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
