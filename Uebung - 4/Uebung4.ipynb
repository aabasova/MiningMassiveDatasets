{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72051a3",
   "metadata": {},
   "source": [
    "# Problem Set 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4424f",
   "metadata": {},
   "source": [
    "Team: \n",
    "    Lea Schmierer 3546563;\n",
    "    Angelina Basova 3704658;\n",
    "    Daniel Knorr 3727033"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85055ee5",
   "metadata": {},
   "source": [
    "## Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b48ab",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606fb41",
   "metadata": {},
   "source": [
    "#### a) Explain briefly why all nodes in the clique have the same PageRank value.\n",
    "\n",
    "All nodes in the clique have the same value, because they all have the same number of in-links and out-links, therefore none of these nodes are more important than the others. Every note is weighted the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea59239",
   "metadata": {},
   "source": [
    "#### b) \n",
    "The Google Matrix A: A= beta* M+(1-beta)*[1/N] with beta = probability to follow a link at random\n",
    "\n",
    "Now: beta= 1 (??????? -> ODER???? oder ist 1-beta= 1????????)\n",
    "\n",
    "MATRIX IN SKRIPT????   -> NEIN DAS AUS DER AUFGABE NOCH ABSCHREIBEN!!! \n",
    "\n",
    "--> A= beta* M+0* [1/N] = beta* M  with beta = probability to follow a link at random\n",
    "\n",
    " Matrix A in the script: \n",
    " A= 1*[\n",
    "       [1/2, 1/2, 0],\n",
    "       [1/2, 0, 0],\n",
    "       [0, 1/2, 1], ]  \n",
    "  = [\n",
    "       [1/2, 1/2, 0],\n",
    "       [1/2, 0, 0],\n",
    "       [0, 1/2, 1], ]\n",
    "\n",
    "\n",
    "NEIINNN SEITE 42 nochmal schauen da steht was dazu \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f246a6",
   "metadata": {},
   "source": [
    "#### c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275817d",
   "metadata": {},
   "source": [
    "#### d) Explain briefly why random teleports not only solve the spider trap problem but also the dead ends problem. \n",
    "\n",
    "There are two approaches to dealing with dead ends: \n",
    "- We can drop the dead ends from the graph and their incoming arcs.\n",
    "- We can modify the process by which random surfers are assumed to move about the web. This approach also resolves the problem of spider traps. It is also called taxation\n",
    "\n",
    "Procedure:\n",
    "If we use the first approach, then we solve the remaining graph G by different appropriate, including the taxation\n",
    "method. \n",
    "We restore the graph and keep the PageRank values. Nodes not in G, but with predecessors all in G can have their PageRank computed by summing, over all predecessors p, the PageRank of p divided by the number of successors of p in the graph. \n",
    "Now there may be other nodes, not in G, that have the PageRank of all their predecessors computed. These may have their own PageRank computed by the same process. Eventually, all nodes outside G will have their PageRank computed; they can surely be computed in the order opposite to that in which\n",
    "they were deleted.\n",
    "\n",
    "Solution: \n",
    "The previous procedure describes how the columns of a matrix M were made stochastic so that teleportation can always be done when there is nowhere else to go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172ffa9",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8746ce",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b48b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType\n",
    "\n",
    "\n",
    "######################\n",
    "# 4a)\n",
    "print(\"Exercise 4a)\")\n",
    "######################\n",
    "\n",
    "sparkSession = SparkSession.builder.appName('A4E4').getOrCreate()\n",
    "\n",
    "print(\"# --------------------------------\")\n",
    "print(\"# Create DataFrame DF1 webStanfordDF\")\n",
    "print(\"# --------------------------------\")\n",
    "\n",
    "rdd_datafile = sparkSession.sparkContext.textFile('./web-Stanford_small.txt')\n",
    "\n",
    "rdd_web_standford = rdd_datafile.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "webStanfordRDD = rdd_web_standford.map(lambda p: (\n",
    "    int(p[0]), int(p[1].strip())))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FromNodeId\", IntegerType(), True),\n",
    "    StructField(\"ToNodeId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "webStanfordDF = sparkSession.createDataFrame(webStanfordRDD, schema)\n",
    "\n",
    "# Get all Nodes as a Set\n",
    "listFromNodeId = webStanfordDF.select(webStanfordDF[\"FromNodeId\"]).toPandas()['FromNodeId']\n",
    "listeToNodeId = webStanfordDF.select(webStanfordDF[\"ToNodeId\"]).toPandas()['ToNodeId']\n",
    "\n",
    "listFromNodeId = list(listFromNodeId)\n",
    "listeToNodeId = list(listeToNodeId)\n",
    "\n",
    "allNodesList = listFromNodeId + listeToNodeId\n",
    "allNodesSet = set(allNodesList)\n",
    "\n",
    "# Define dictionary\n",
    "dic = {}\n",
    "\n",
    "def generateDictionary(allNodesSet, webStanfordDF, dic):\n",
    "    for node in allNodesSet:\n",
    "        liste = []\n",
    "        filteredByFromNodeId = webStanfordDF.filter(webStanfordDF[\"FromNodeId\"] == node)\n",
    "        listOutgoingLinksTo = filteredByFromNodeId.select(filteredByFromNodeId[\"ToNodeId\"]).toPandas()['ToNodeId']\n",
    "        filteredByToNodeId = webStanfordDF.filter(webStanfordDF[\"ToNodeId\"] == node)\n",
    "        listIngoingLinksFrom = filteredByToNodeId.select(filteredByToNodeId[\"FromNodeId\"]).toPandas()[\n",
    "            'FromNodeId']\n",
    "        listOutgoingLinksTo = list(listOutgoingLinksTo)\n",
    "        listIngoingLinksFrom = list(listIngoingLinksFrom)\n",
    "        liste.append(listOutgoingLinksTo)\n",
    "        liste.append(listIngoingLinksFrom)\n",
    "        dic[node] = liste\n",
    "\n",
    "\n",
    "generateDictionary(allNodesSet, webStanfordDF, dic)\n",
    "\n",
    "print(\"Dictionary Composition: Node i: <[Out-neighbors of i]>,<[In-neighbors of i]>\")\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "sparkSession = SparkSession.builder.appName('A4E4').getOrCreate()\n",
    "\n",
    "######################\n",
    "# 4b)\n",
    "print(\"Exercise 4b)\")\n",
    "######################\n",
    "\n",
    "rdd_datafile = sparkSession.sparkContext.textFile('./web-Stanford_small.txt')\n",
    "\n",
    "rdd_web_standford = rdd_datafile.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FromNodeId\", StringType(), False),\n",
    "    StructField(\"ToNodeId\", StringType(), False)\n",
    "])\n",
    "\n",
    "webStanfordDF = sparkSession.createDataFrame(rdd_web_standford, schema)\n",
    "\n",
    "#create FromNode_list\n",
    "FromNode_list = webStanfordDF.rdd.map(lambda x: x.FromNodeId).collect()\n",
    "\n",
    "# create ToNode_set\n",
    "ToNode_list = webStanfordDF.rdd.map(lambda x: x.ToNodeId).collect()\n",
    "ToNode_set = set(ToNode_list)\n",
    "\n",
    "# die ganz Toten finden (die, die nicht woanders hinzeigen)\n",
    "dead_ends = set()\n",
    "\n",
    "for ToNode in ToNode_set:\n",
    "    if ToNode not in FromNode_list:\n",
    "        dead_ends.add(ToNode)\n",
    "\n",
    "# die Toten finden, die nur auf die Toten zeigen\n",
    "# python gesamtliste mit Pandas erzeugen\n",
    "pdframe = webStanfordDF.toPandas()\n",
    "gesamtliste = list(pdframe.values.tolist())\n",
    "gesamtliste.sort()\n",
    "\n",
    "neuToteSuchen = True\n",
    "while neuToteSuchen:\n",
    "    neue_dead_ends = set()\n",
    "    FromNodeOld = \"\"\n",
    "    countNotDead = 0\n",
    "    for x in gesamtliste:\n",
    "        FromNode = x[0]\n",
    "        ToNode = x[1]\n",
    "        if FromNode != FromNodeOld:  # gruppenwechsel\n",
    "            if FromNodeOld != \"\":    # beim erstem mal nicht\n",
    "                if countNotDead == 0:\n",
    "                    dead_ends.add(FromNodeOld)\n",
    "                    neue_dead_ends.add(FromNodeOld)\n",
    "            FromNodeOld = FromNode\n",
    "            countNotDead = 0\n",
    "        if ToNode not in dead_ends:\n",
    "            countNotDead = countNotDead + 1\n",
    "\n",
    "    if len(neue_dead_ends) == 0:\n",
    "        neuToteSuchen = False   # keine mehr gefunden - das wars\n",
    "\n",
    "    workliste = [x for x in gesamtliste if x[0] not in neue_dead_ends]\n",
    "    gesamtliste = workliste\n",
    "\n",
    "print(\"Anzahl DEAD-ENDS: \", len(dead_ends))\n",
    "print(\"Dead End Liste: \", dead_ends)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
