{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/04 23:36:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- artist_id: integer (nullable = true)\n",
      " |-- play_count: integer (nullable = true)\n",
      "\n",
      "+-------+---------+----------+\n",
      "|user_id|artist_id|play_count|\n",
      "+-------+---------+----------+\n",
      "|1059637|  1000010|       238|\n",
      "|1059637|  1000049|         1|\n",
      "|1059637|  1000056|         1|\n",
      "|1059637|  1000062|        11|\n",
      "|1059637|  1000094|         1|\n",
      "|1059637|  1000112|       423|\n",
      "|1059637|  1000113|         5|\n",
      "|1059637|  1000114|         2|\n",
      "|1059637|  1000123|         2|\n",
      "|1059637|  1000130|     19129|\n",
      "|1059637|  1000139|         4|\n",
      "|1059637|  1000241|       188|\n",
      "|1059637|  1000263|       180|\n",
      "|1059637|  1000289|         2|\n",
      "|1059637|  1000305|         1|\n",
      "|1059637|  1000320|        21|\n",
      "|1059637|  1000340|         1|\n",
      "|1059637|  1000427|        20|\n",
      "|1059637|  1000428|        12|\n",
      "|1059637|  1000433|        10|\n",
      "+-------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(\"dataset-problemset2-ex5/user_artist_data_small.txt\", delim_whitespace=True, names=['user_id', 'artist_id', 'play_count'])\n",
    "\n",
    "#df\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), False), \\\n",
    "    StructField(\"artist_id\", IntegerType(), False), \\\n",
    "    StructField(\"play_count\", IntegerType(), True)\\\n",
    "        ])\n",
    "    \n",
    "\n",
    "df = spark.read.csv(\"dataset-problemset2-ex5/user_artist_data_small.txt\", sep=' ', schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Populate a utility matrix. Be sure to first replace bad artist ids that are due to known misspellings using the assignments in artist_alias_small.txt. Think about how to store the matrix in a reasonable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- misspelled_name: integer (nullable = true)\n",
      " |-- correct_name: integer (nullable = true)\n",
      "\n",
      "+---------------+------------+\n",
      "|misspelled_name|correct_name|\n",
      "+---------------+------------+\n",
      "|        1027859|     1252408|\n",
      "|        1017615|         668|\n",
      "|        6745885|     1268522|\n",
      "|        1018110|     1018110|\n",
      "|        1014609|     1014609|\n",
      "|        6713071|        2976|\n",
      "|        1014175|     1014175|\n",
      "|        1008798|     1008798|\n",
      "|        1013851|     1013851|\n",
      "|        6696814|     1030672|\n",
      "|        1036747|     1239516|\n",
      "|        1278781|     1021980|\n",
      "|        2035175|     1007565|\n",
      "|        1327067|     1308328|\n",
      "|        2006482|     1140837|\n",
      "|        1314530|     1237371|\n",
      "|        1160800|     1345290|\n",
      "|        1255401|     1055061|\n",
      "|        1307351|     1055061|\n",
      "|        1234249|     1005225|\n",
      "+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace bad artists e.g. 6979261\n",
    "\n",
    "schema_artists = StructType([\\\n",
    "    StructField(\"misspelled_name\", IntegerType(), False),\n",
    "    StructField(\"correct_name\", IntegerType(), False)])\n",
    "\n",
    "false_to_correct_artists = spark.read.csv(\n",
    "    \"dataset-problemset2-ex5/artist_alias_small.txt\", sep='\\t', schema=schema_artists)\n",
    "\n",
    "false_to_correct_artists.printSchema()\n",
    "\n",
    "false_to_correct_artists.show()\n",
    "\n",
    "#false_to_correct_artists.show()\n",
    "\n",
    "\n",
    "#return from misspelled artist the correctly speller artist. I assume that \"artist_alia_small.txt\" is in the format <misspelled artist> <correctly spelled artist>\n",
    "def get_correct_artist(misspelled_artist: int, false_to_correct_artists):\n",
    "    df = false_to_correct_artists.filter(\n",
    "        misspelled_artist == false_to_correct_artists[0]).collect()\n",
    "\n",
    "        # if no artist was returned then the artist was already correctly spelled\n",
    "    if len(df) == 0:\n",
    "        return misspelled_artist\n",
    "    else:\n",
    "        correct_artist = df[0][1]\n",
    "        return correct_artist\n",
    "\n",
    "\n",
    "# pack above method in a udf, with the hope that the map function will run without bugs. it did't work :(\n",
    "correct_artist = udf(lambda x: get_correct_artist(\n",
    "    x, false_to_correct_artists), IntegerType())\n",
    "\n",
    "#the actual mapping in the second column from misspelled to correctly spelled\n",
    "rdd_clean = df.rdd.map(lambda x: (x[0], correct_artist(x[1]), x[2]))\n",
    "\n",
    "\n",
    "columns = ['user_id', 'artist_id', 'play_count']\n",
    "\n",
    "\n",
    "# transforming rdd to dataframe causes error that i cant solve\n",
    "#df_clean = rdd_clean.toDF(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034635"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_correct_artist(1002670, false_to_correct_artists)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
